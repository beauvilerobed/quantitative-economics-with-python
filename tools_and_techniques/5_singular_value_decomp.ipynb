{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGULAR VALUE DECOMPOSITION (SVD)\n",
    "\n",
    "# Content\n",
    "- four fundamental spaces of linear algebra\n",
    "- under-determined and over-determined least squares regressions\n",
    "- principal components analysis (PCA)\n",
    "\n",
    "The singular value decomposition (SVD) is a work-horse in applications of least squares projection that form foundations for many statistical and machine learning methods. After defining the SVD, we’ll describe how it connects to four fundamental spaces of linear algebra, under-determined and over-determined least squares regressions, and principal components analysis (PCA).\n",
    "\n",
    "## The Setting\n",
    "\n",
    "Let $X$ be an $m\\times n$ matrix of rank $p$.\n",
    "\n",
    "Necessarily, $p\\leq min(m,n)$. In much of this notebook, we’ll think of $X$ as a matrix of data in which\n",
    "- each column is an **individual** – a time period or person, depending on the application\n",
    "- each row is a **random variable** describing an attribute of a time period or a person, depending on the application\n",
    "\n",
    "We’ll be interested in two situations\n",
    "- A short and fat case in which $m << n$, so that there are many more columns (individuals) than rows (attributes).\n",
    "- A tall and skinny case in which $m >> n$, so that there are many more rows (attributes) than columns (individuals).\n",
    "\n",
    "We’ll apply a **singular value decomposition** of $X$ in both situations. \n",
    "\n",
    "In the $m << n$ case in which there are many more individuals $n$ than attributes $m$, we can calculate sample moments of a joint distribution by taking averages across observations of functions of the observations.\n",
    "\n",
    "In this $m << n$ case, we’ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the $m >> n$ case in which there are many more attributes than individuals $n$ and when we are in a time-series setting in which $n$ equals the number of time periods covered in the data set $X$, we’ll proceed in a different way. We’ll again use a singular value decomposition, but now to construct a **dynamic mode decomposition** (DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $m\\times n$ matrix $X$ of rank $p\\leq min(m,n)$ is\n",
    "$$\n",
    "\\begin{equation}\n",
    "X = U\\Sigma V^T\n",
    "\\end{equation}\\tag{5.1}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "UU^T = I, U^TU = I\n",
    "$$\n",
    "$$\n",
    "VV^T = I, V^TV = I\n",
    "$$\n",
    "and\n",
    "- $U$ is an $m\\times m$ orthogonal matrix of **left singular vectors** of $X$\n",
    "- Columns of $U$ are eigenvectors of $XX^T$\n",
    "- $V$ is an $n\\times n$ orthogonal matrix of r**ight singular vectors** of $X$\n",
    "- Columns of $V$ are eigenvectors of $X^TX$\n",
    "- $\\Sigma$ is an $m\\times n$ matrix in which the first $p$ places on its main diagonal are positive numbers $\\sigma_1, \\sigma_2,...,\\sigma_p$ called **singular values**; remaining entries of $\\Sigma$ are all zero\n",
    "- The $p$ singular values are positive square roots of the eigenvalues of the $m\\times m$ matrix $XX^T$ and also of the $n\\times n$ matrix $X^TX$ \n",
    "- We adopt a convention that when $U$ is a complex valued matrix, $U^T$ denotes the **conjugate-transpose** or **Hermitian-transpose** of $U$, meaning that $U_{ij}^T$ is the complex conjugate of $U_{ji}$.\n",
    "- Similarly, when $V$ is a complex valued matrix, $V^T$ denotes the **conjugate-transpose** or **Hermitian-transpose** of $V$\n",
    "\n",
    "The matrices $U, \\Sigma, V$ entail linear transformations that reshape in vectors in the following ways:\n",
    "\n",
    "- multiplying vectors by the unitary matrices $U$ and $V$ **rotates** them, but leaves **angles between vectors** and **lengths of vectors** unchanged.\n",
    "\n",
    "- multiplying vectors by the diagonal matrix $\\Sigma$ leaves **angles between vectors** unchanged but **rescales** vectors.\n",
    "\n",
    "Thus, representation (5.1) asserts that multiplying an $n\\times 1$ vector $y$ by the matrix $m\\times n$ amounts to performing the following three multiplications of $y$ sequentially:\n",
    "\n",
    "- **rotating** $y$ by computing $V^Ty$\n",
    "- **rescaling** $V^Ty$ by multiplying it by $\\Sigma$\n",
    "- **rotating** $\\Sigma V^Ty$ by multiplying it by $U$\n",
    "\n",
    "This structure of the $m\\times n$ matrix $X$ opens the door to constructing systems of data **encoders** and **decoders**.\n",
    "\n",
    "Thus,\n",
    "\n",
    "- $V^Ty$ is an encoder\n",
    "- $\\Sigma$ is an operator to be applied to the encoded data\n",
    "- $U$ is a decoder to be applied to the output from applying operator $\\Sigma$ to the encoded data\n",
    "\n",
    "We’ll apply this circle of ideas later in this notebook when we study Dynamic Mode Decomposition.\n",
    "\n",
    "**Road Ahead**\n",
    "\n",
    "What we have described above is called a **full** SVD.\n",
    "\n",
    "In a full SVD, the shapes of $U, \\Sigma$, and $V$ are $(m,m)$, $(m,n)$, $(n,n)$, respectively. Later we’ll also describe an **economy** or **reduced** SVD.\n",
    "\n",
    "Before we study a **reduced** SVD we’ll say a little more about properties of a **full** SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four Fundamental Subspaces\n",
    "\n",
    "Let $\\mathcal C$ denote a column space, $\\mathcal N$ denote a null space, and $\\mathcal R$ denote a row space.\n",
    "\n",
    "Let’s start by recalling the four fundamental subspaces of an $m\\times n$ matrix $X$ of rank $p$.\n",
    "- The **column space** of $X$, denoted $\\mathcal C(X)$, is the span of the columns of $X$, i.e., all vectors $y$ that can be written as linear combinations of columns of $X$. Its dimension is $p$.\n",
    "- The **null space** of $X$, denoted $\\mathcal N(X)$ consists of all vectors $y$ that satisfy $Xy = 0$. Its dimension is $n-p$.\n",
    "- The **row space** of $X$, denoted $\\mathcal R(X)$ is the column space of $X^T$. It consists of all vectors $z$ that can be written as linear combinations of rows of $X$. Its dimension is $p$.\n",
    "- The l**eft null space** of $X$, denoted $\\mathcal N(X^T)$, consist of all vectors $z$ such that $X^Tz = 0$. Its dimension is $m-p$.\n",
    "\n",
    "For a full SVD of a matrix $X$, the matrix $U$ of **left singular vectors** and the matrix $V$ of **right singular vectors** contain orthogonal bases for all four subspaces.\n",
    "\n",
    "They form two pairs of orthogonal subspaces that we’ll describe now.\n",
    "\n",
    "Let $u_i, i=1,...,m$ be the $m$ column vectors of $U$ and let $v_i, i=1,...,n$ be the $n$ column vectors of $V$.\n",
    "\n",
    "Let’s write the full SVD of $X$ as\n",
    "$$\n",
    "\\begin{equation}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "U_L & U_R\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_p & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "V_L & V_R\n",
    "\\end{bmatrix}^T \n",
    "\\end{equation}\\tag{5.2}\n",
    "$$\n",
    "\n",
    "where $\\Sigma_p$ is a $p\\times p$ diagonal matrix with the $p$ singular values on the diagonal and\n",
    "\n",
    "$$\n",
    "U_L = \n",
    "\\begin{bmatrix}\n",
    "u_1 & \\dots & u_p\n",
    "\\end{bmatrix}, \n",
    "U_R = \n",
    "\\begin{bmatrix}\n",
    "u_{p+1} & \\dots & u_m\n",
    "\\end{bmatrix}, \n",
    "$$\n",
    "$$\n",
    "V_L = \n",
    "\\begin{bmatrix}\n",
    "v_1 & \\dots & v_p\n",
    "\\end{bmatrix}, \n",
    "V_R = \n",
    "\\begin{bmatrix}\n",
    "v_{p+1} & \\dots & v_n\n",
    "\\end{bmatrix}, \n",
    "$$\n",
    "\n",
    "Representation (5.2) implies that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X\n",
    "\\begin{bmatrix}\n",
    "V_L & V_R\n",
    "\\end{bmatrix}\n",
    " = \n",
    "\\begin{bmatrix}\n",
    "U_L & U_R\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_p & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\\tag{5.2}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "XV_L = U_L\\Sigma_p\\\\\n",
    "XV_R = 0\n",
    "\\end{align}\\tag{5.3}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\begin{align}\n",
    "Xv_i = \\sigma_i u_i, i=1,...,p\\\\\n",
    "Xv_i = 0, i=p+1,...,n\n",
    "\\end{align}\\tag{5.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations (5.4) tell how the transformation $X$ maps a pair of orthonormal vectors $v_i$, $v_j$ for $i$ and $j$ both less than or equal to the rank $p$ of $X$ into a pair of orthonormal vectors $u_i$, $u_j$. Equations (5.3) assert that\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal C(X) = \\mathcal C(U_L)\\\\\n",
    "\\mathcal N(X) = \\mathcal C(V_R)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking transposes on both sides of representation (5.2) implies\n",
    "\n",
    "$$\n",
    "X^T\n",
    "\\begin{bmatrix}\n",
    "U_L & U_R\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "V_L & V_R\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_p & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X^TU_L = V_L\\Sigma_p\\\\\n",
    "X^TU_R = 0\n",
    "\\end{align}\\tag{5.5}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X^Tu_i = \\sigma_i v_i, i=1,...,p\\\\\n",
    "X^Tu_i = 0, i=p+1,...,m\n",
    "\\end{align}\\tag{5.6}\n",
    "$$\n",
    "\n",
    "Notice how equations (5.6) assert that the transformation $X^T$ maps a pair of distinct orthonormal vectors $u_i$, $u_j$ for $i$ and $j$ both less than or equal to the rank $p$ of $X$ into a pair of orthonormal vectors $v_i$, $v_j$. Equations (5.5) assert that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal R(X) \\equiv \\mathcal C(X^T) = \\mathcal C(V_L)\\\\\n",
    "\\mathcal N(X^T) = \\mathcal C(U_R)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, taken together, the systems of equations (5.3) and (5.5) describe the four fundamental subspaces of $X$ in the following ways:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal C(X) = \\mathcal C(U_L)\\\\\n",
    "\\mathcal N(X^T) = \\mathcal C(U_R)\\\\\n",
    "\\mathcal R(X) \\equiv \\mathcal C(X^T) = \\mathcal C(V_L)\\\\\n",
    "\\mathcal N(X) = \\mathcal C(V_R)\\\\\n",
    "\\end{align}\\tag{5.7}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $U$ and $V$ are both orthonormal matrices, collection (5.7) asserts that\n",
    "- $U_L$ is an orthonormal basis for the column space of $X$\n",
    "- $U_L$ is an orthonormal basis for the null space of $X^T$\n",
    "- $V_L$ is an orthonormal basis for the row space of $X$\n",
    "- $V_R$ is an orthonormal basis for the null space of $X$\n",
    "\n",
    "We have verified the four claims in (5.7) simply by performing the multiplications called for by the right side of (5.2) and reading them.\n",
    "\n",
    "The claims in (5.7) and the fact that $U$ and $V$ are both unitary (i.e, orthonormal) matrices imply that\n",
    "- the column space of $X$ is orthogonal to the null space of $X^T$\n",
    "- the null space of $X$ is orthogonal to the row space of $X$\n",
    "\n",
    "Sometimes these properties are described with the following two pairs of orthogonal complement subspaces:\n",
    "- $\\mathcal C(X)$ is the orthogonal complement of $\\mathcal N(X^T)$ \n",
    "- $\\mathcal R(X)$ is the orthogonal complement of $\\mathcal N(X)$ \n",
    "\n",
    "Let’s do an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of matrix:\n",
      " 2\n",
      "S: \n",
      " [2.69e+01 1.86e+00 8.37e-16 5.83e-16 4.43e-17]\n",
      "U:\n",
      " [[-0.27 -0.73  0.55  0.31  0.04]\n",
      " [-0.35 -0.42 -0.29 -0.78 -0.07]\n",
      " [-0.43 -0.11 -0.65  0.46  0.42]\n",
      " [-0.51  0.19 -0.06  0.22 -0.81]\n",
      " [-0.59  0.5   0.44 -0.19  0.41]]\n",
      "Column space:\n",
      " [[-0.27 -0.35]\n",
      " [ 0.73  0.42]\n",
      " [ 0.14 -0.29]\n",
      " [ 0.53 -0.77]\n",
      " [ 0.32  0.14]]\n",
      "Left null space:\n",
      " [[ 0.55  0.31  0.04]\n",
      " [-0.29 -0.78 -0.07]\n",
      " [-0.65  0.46  0.42]\n",
      " [-0.06  0.22 -0.81]\n",
      " [ 0.44 -0.19  0.41]]\n",
      "V.T:\n",
      " [[-0.27  0.73  0.14  0.53  0.32]\n",
      " [-0.35  0.42 -0.29 -0.77  0.14]\n",
      " [-0.43  0.11 -0.25  0.2  -0.84]\n",
      " [-0.51 -0.19  0.81 -0.19 -0.05]\n",
      " [-0.59 -0.5  -0.41  0.23  0.42]]\n",
      "Row space:\n",
      " [[-0.27 -0.35 -0.43 -0.51 -0.59]\n",
      " [-0.73 -0.42 -0.11  0.19  0.5 ]]\n",
      "Right null space:\n",
      " [[-0.43  0.11 -0.25  0.2  -0.84]\n",
      " [-0.51 -0.19  0.81 -0.19 -0.05]\n",
      " [-0.59 -0.5  -0.41  0.23  0.42]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Define the matrix\n",
    "A = np.array([[1, 2, 3, 4, 5],\n",
    "              [2, 3, 4, 5, 6],\n",
    "              [3, 4, 5, 6, 7],\n",
    "              [4, 5, 6, 7, 8],\n",
    "              [5, 6, 7, 8, 9]])\n",
    "\n",
    "# Compute the SVD of the matrix\n",
    "U, S, V = np.linalg.svd(A,full_matrices=True)\n",
    "\n",
    "# Compute the rank of the matrix\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "\n",
    "# Print the rank of the matrix\n",
    "print(\"Rank of matrix:\\n\", rank)\n",
    "print(\"S: \\n\", S)\n",
    "\n",
    "# Compute the four fundamental subspaces\n",
    "row_space = U[:, :rank]\n",
    "col_space = V[:, :rank]\n",
    "null_space = V[:, rank:]\n",
    "left_null_space = U[:, rank:]\n",
    "\n",
    "\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Column space:\\n\", col_space)\n",
    "print(\"Left null space:\\n\", left_null_space)\n",
    "print(\"V.T:\\n\", V.T)\n",
    "print(\"Row space:\\n\", row_space.T)\n",
    "print(\"Right null space:\\n\", null_space.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eckart-Young Theorem\n",
    "\n",
    "Suppose that we want to construct the best rank $r$ approximation of an $m\\times n$ matrix $X$. By best, we mean a matrix $X_r$ of rank $r < p$ that, among all rank $r$ matrices, minimizes\n",
    "\n",
    "$$\n",
    "\\Vert X - X_r\\Vert\n",
    "$$\n",
    "\n",
    "where $\\Vert\\cdot\\Vert$ denotes a norm of a matrix $X$ and where $X_r$ belongs to the space of all rank $r$ matrices of dimension $m\\times n$. Three popular **matrix norms** of an $m\\times n$ matrix $X$ can be expressed in terms of the singular values of $X$\n",
    "- the **spectral** or $l^2$ norm $\\Vert X\\Vert_2 = \\max_{\\Vert y\\Vert \\neq 0}\\frac{\\Vert Xy\\Vert}{\\Vert y\\Vert} = \\sigma_1$\n",
    "- the **Frobenius** norm $\\Vert X\\Vert_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_p^2}$\n",
    "- the **nuclear** norm $\\Vert X\\Vert_N = \\sigma_1 + \\cdots + \\sigma_p$ \n",
    "\n",
    "The Eckart-Young theorem states that for each of these three norms, same rank $r$ matrix is best and that it equals\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{X}_r = \\sigma_1U_1V^T_1 + \\sigma_2U_2V^T_2 + \\cdots + \\sigma_rU_rV^T_r\n",
    "\\end{equation}\\tag{5.8}\n",
    "$$\n",
    "\n",
    "This is a very powerful theorem that says that we can take our $m\\times n$ matrix $X$ that in not full rank, and we can best approximate it by a full rank $p\\times p$ matrix through the SVD.\n",
    "\n",
    "Moreover, if some of these $p$ singular values carry more information than others, and if we want to have the most amount of information with the least amount of data, we can take $r$ leading singular values ordered by magnitude.\n",
    "\n",
    "We’ll say more about this later when we present Principal Component Analysis. You can read about the Eckart-Young theorem and some of its uses [here](https://en.wikipedia.org/wiki/Low-rank_approximation). We’ll make use of this theorem when we discuss principal components analysis (PCA) and also dynamic mode decomposition (DMD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full and Reduced SVD’s\n",
    "\n",
    "Up to now we have described properties of a **full** SVD in which shapes of $U$, $\\Sigma$, and $V$ are $(m,m)$, $(m,n)$, $(n,n)$, respectively.\n",
    "\n",
    "There is an alternative bookkeeping convention called an **economy** or **reduced** SVD in which the shapes of $U$, $\\Sigma$, and $V$  are different from what they are in a full SVD.\n",
    "\n",
    "Thus, note that because we assume that $X$ has rank $p$, there are only $p$ nonzero singular values, where $p = rank(X)\\leq \\min(m,n)$.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $U$, $\\Sigma$, and $V$ as matrices with shapes $(m,p)$, $(p,p)$, $(n,p)$.\n",
    "\n",
    "You can read about reduced and full SVD [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html).\n",
    "\n",
    "For a full SVD,\n",
    "\n",
    "$$\n",
    "UU^T = I, U^TU = I\n",
    "$$\n",
    "$$\n",
    "VV^T = I, V^TV = I\n",
    "$$\n",
    "\n",
    "But not all these properties hold for a **reduced** SVD. Which properties hold depend on whether we are in a **tall-skinny** case or a **short-fat** case.\n",
    "\n",
    "- In a tall-skinny case in which $m >> n$, for a **reduced** SVD\n",
    "$$\n",
    "UU^T \\neq I, U^TU = I\n",
    "$$\n",
    "$$\n",
    "VV^T = I, V^TV = I\n",
    "$$\n",
    " \n",
    "- In a short-fat case in which $m << n$, for a **reduced** SVD\n",
    "$$\n",
    "UU^T = I, U^TU = I\n",
    "$$\n",
    "$$\n",
    "VV^T = I, V^TV \\neq I\n",
    "$$\n",
    "\n",
    "When we study Dynamic Mode Decomposition below, we shall want to remember these properties when we use a reduced SVD to compute some DMD representations. Let’s do an exercise to compare **full** and **reduced** SVD’s.\n",
    "\n",
    "To review,\n",
    "\n",
    "- in a **full** SVD\n",
    "    - $U$ is $m\\times m$ \n",
    "    - $\\Sigma$ is $m\\times n$ \n",
    "    - $V$ is $n\\times n$ \n",
    "\n",
    "- in a **reduced** SVD\n",
    "    - $U$ is $m\\times p$ \n",
    "    - $\\Sigma$ is $p\\times p$ \n",
    "    - $V$ is $n\\times p$ \n",
    "\n",
    "First, let’s study a case in which $m=5>n=2$. (This is a small example of the **tall-skinny** case that will concern us when we study **Dynamic Mode Decompositions** below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S, V =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.5 ,  0.58, -0.6 , -0.2 ,  0.15],\n",
       "        [-0.03, -0.21, -0.12, -0.69, -0.68],\n",
       "        [-0.66,  0.13,  0.72, -0.15,  0.02],\n",
       "        [-0.54, -0.49, -0.29,  0.54, -0.32],\n",
       "        [-0.17, -0.61, -0.14, -0.41,  0.64]]),\n",
       " array([1.91, 0.2 ]),\n",
       " array([[-0.78, -0.63],\n",
       "        [-0.63,  0.78]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V =')\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uhat, Shat, Vhat = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.5 ,  0.58],\n",
       "        [-0.03, -0.21],\n",
       "        [-0.66,  0.13],\n",
       "        [-0.54, -0.49],\n",
       "        [-0.17, -0.61]]),\n",
       " array([1.91, 0.2 ]),\n",
       " array([[-0.78, -0.63],\n",
       "        [-0.63,  0.78]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uhat, Shat, Vhat = ')\n",
    "Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank of X = 2\n"
     ]
    }
   ],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print(f'rank of X = {rr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties:\n",
    "\n",
    "- Where $U$ is constructed via a full SVD, $U^TU = I_{m\\times m}$ and $UU^T = I_{m\\times m}$\n",
    "- Where $\\hat{U}$ is constructed via a reduced SVD, although $\\hat{U}^T\\hat{U} = I_{p\\times p}$, it happens that $\\hat{U}\\hat{U}^T \\neq I_{m\\times m}$\n",
    "\n",
    "We illustrate these properties for our example with the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUT, UTU = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00e+00, -1.74e-16,  5.20e-18, -6.14e-17, -6.59e-17],\n",
       "        [-1.74e-16,  1.00e+00, -4.26e-17,  1.99e-16,  9.80e-17],\n",
       "        [ 5.20e-18, -4.26e-17,  1.00e+00, -1.49e-17, -2.86e-17],\n",
       "        [-6.14e-17,  1.99e-16, -1.49e-17,  1.00e+00,  2.79e-17],\n",
       "        [-6.59e-17,  9.80e-17, -2.86e-17,  2.79e-17,  1.00e+00]]),\n",
       " array([[ 1.00e+00, -1.54e-17, -4.01e-18,  9.81e-18,  3.07e-17],\n",
       "        [-1.54e-17,  1.00e+00, -1.64e-17,  1.63e-16,  2.37e-16],\n",
       "        [-4.01e-18, -1.64e-17,  1.00e+00,  5.41e-17,  3.88e-17],\n",
       "        [ 9.81e-18,  1.63e-16,  5.41e-17,  1.00e+00,  4.53e-17],\n",
       "        [ 3.07e-17,  2.37e-16,  3.88e-17,  4.53e-17,  1.00e+00]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTU = U.T@U\n",
    "UUT = U@U.T\n",
    "print('UUT, UTU = ')\n",
    "UUT, UTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UhatUhatT, UhatTUhat= \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.58, -0.11,  0.4 , -0.01, -0.27],\n",
       "        [-0.11,  0.05, -0.01,  0.12,  0.13],\n",
       "        [ 0.4 , -0.01,  0.45,  0.29,  0.03],\n",
       "        [-0.01,  0.12,  0.29,  0.53,  0.39],\n",
       "        [-0.27,  0.13,  0.03,  0.39,  0.4 ]]),\n",
       " array([[ 1.00e+00, -1.54e-17],\n",
       "        [-1.54e-17,  1.00e+00]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UhatUhatT = Uhat@Uhat.T\n",
    "UhatTUhat = Uhat.T@Uhat\n",
    "print('UhatUhatT, UhatTUhat= ')\n",
    "UhatUhatT, UhatTUhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "The cells above illustrate the application of the `full_matrices=True` and `full_matrices=False` options. Using `full_matrices=False` returns a reduced singular value decomposition.\n",
    "\n",
    "The **full** and **reduced** SVD’s both accurately decompose an $m\\times n $ matrix $X$\n",
    "\n",
    "When we study Dynamic Mode Decompositions below, it will be important for us to remember the preceding properties of full and reduced SVD’s in such tall-skinny cases.\n",
    "\n",
    "Now let’s turn to a short-fat case.\n",
    "\n",
    "To illustrate this case, we’ll set $m=2<5=n$ and compute both full and reduced SVD’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S, V = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.8, -0.6],\n",
       "        [-0.6,  0.8]]),\n",
       " array([1.35, 0.58]),\n",
       " array([[-0.3 , -0.07, -0.6 , -0.49, -0.55],\n",
       "        [-0.34,  0.22,  0.01, -0.61,  0.69],\n",
       "        [-0.52, -0.29,  0.72, -0.17, -0.32],\n",
       "        [-0.71,  0.32, -0.22,  0.58,  0.06],\n",
       "        [-0.15, -0.87, -0.27,  0.15,  0.35]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V = ')\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uhat, Shat, Vhat = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.8, -0.6],\n",
       "        [-0.6,  0.8]]),\n",
       " array([1.35, 0.58]),\n",
       " array([[-0.3 , -0.07, -0.6 , -0.49, -0.55],\n",
       "        [-0.34,  0.22,  0.01, -0.61,  0.69]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uhat, Shat, Vhat = ')\n",
    "Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s verify that our reduced SVD accurately represents $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SShat=np.diag(Shat)\n",
    "np.allclose(X, Uhat@SShat@Vhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Decomposition\n",
    "\n",
    "A **reduced** singular value decomposition (SVD) of $X$ is related to a **polar decomposition** of $X$\n",
    "$$\n",
    "X = SQ\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "S = U\\Sigma U^T\n",
    "$$\n",
    "$$\n",
    "Q = UV^T\n",
    "$$\n",
    "\n",
    "Here\n",
    "- $S$ is an $m\\times m$ **symmetric** matrix\n",
    "- $Q$ is an $m\\times n$ **orthogonal** matrix\n",
    "\n",
    "and in our reduced SVD\n",
    "- $U$ is an $m\\times p$ orthonormal matrix\n",
    "- $\\Sigma$ is a $p\\times p$ diagonal matrix\n",
    "- $V$ is an $n\\times p$ orthonormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Principal Components Analysis (PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
